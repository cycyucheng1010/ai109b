# 20210527第十四週筆記
## 人工智慧的方法
1. 比對法
* 紀錄問題與答案配對後，直接從表格內查出。
* Ex: Elisa
2. 推理法
* 撰寫規則後，電腦根據規則推論。
* Ex: 專家系統
3. 搜尋法
* 對所有可能的結果進行系統式的列舉，然後看看有沒有答案。
* Ex: 深度優先、廣度優先、電腦下棋
4. 統計法
* 找出機率最大的解答。
* Ex: 利用電腦亂數驗證中央極限定理
5. 優化法
* 對每個可能的解答，都給一個分數及權重，找出總分最好的解答。
* Ex: 爬山演算法、遺傳演算法等.....
## 常見神經元的開關函數
![activation function](https://user-images.githubusercontent.com/62127656/120191675-cf878400-c24c-11eb-84e8-91a9d9ceef0d.PNG)
## 深度學習的神經網路
### 捲積神經網路CNN
* 常使用於影像辨識
* 捲積層CONV
![conv](https://user-images.githubusercontent.com/62127656/120192884-54bf6880-c24e-11eb-8b46-1c201bcff04d.PNG)
---
### 循環神經網路RNN, LSTM
* 最常用來處理語言
* Ex: 機器翻譯系統
![RNN](https://user-images.githubusercontent.com/62127656/120197498-96064700-c253-11eb-852a-6602749ec1d2.PNG)
![RNN-1](https://user-images.githubusercontent.com/62127656/120197661-c77f1280-c253-11eb-8886-dc0cb660dbe9.PNG)
![LSTM](https://user-images.githubusercontent.com/62127656/120198054-4aa06880-c254-11eb-95eb-6351a2a749c1.PNG)
### 生成對抗網路GAN
* 採用偉造者與鑑賞者的對忼模式，讓雙方在對抗的過程中能力越來越強。
* 擅長模仿他人風格
![GNN1](https://user-images.githubusercontent.com/62127656/120198884-33ae4600-c255-11eb-89e0-50d95ac5174f.PNG)
### 強化學習機制Reinforcement Learning
* 利用獎勵及扣分的機制去學習
* Ex: AlpaGo圍棋程式
* 基本的強化學習被建模為馬爾可夫決策過程：
  1. 環境狀態的集合S
  2. 動作的集合A
  3. 在狀態之間轉換的規則（轉移概率矩陣）P
  4. 規定轉換後「即時獎勵」的規則（獎勵函數）R
  5. 描述主體能夠觀察到什麼的規則。
  
 ![Reinforcement_learning_diagram svg (1)](https://user-images.githubusercontent.com/62127656/120202364-18453a00-c259-11eb-8d35-9a7561c0a2aa.png)

## 參考資料
* [人工智慧與神經網路 (還有深度學習的進展)](https://www.slideshare.net/ccckmit/ss-94563680)
* [CNN](https://cs.stanford.edu/people/karpathy/convnetjs/)
* [ConvNetJS MNIST demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)
* [ConvNetJS CIFAR-10 demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)
* [pytorch tutorial](https://github.com/yunjey/pytorch-tutorial)
