# 20210422第九週筆記
## 神經網路介紹
* 在電腦領域，神經網路是指一種模擬神經系統所設計出來的程式，用來模擬人類視覺、聽覺等等智慧行為的原理，企圖讓電腦可以具有人類智慧的一種方法。
  下圖是生物神經細胞的結構圖，這個圖看來頗為複雜，如果電腦程式真的要模擬這麼複雜的結構，那程式應該也會非常複雜才對。
![NeuralCell](https://user-images.githubusercontent.com/62127656/115642638-e97c9f80-a34d-11eb-8a10-628d64774424.jpg)
* 好、神經網路程式不需要去模擬「細胞膜、粒線體、核醣體」等等複雜的結構，因為學電腦的人可以透過「抽象化」這個伎倆，將上述的神經細胞結構簡化成下圖 (a) 的樣子。
  在下圖中，a1 ... an 是輸入，w1 ... wn 是權重，這些輸入乘上權重之後加總(SUM)，就會得到神經元的刺激強度，接著經過函數 f() 轉換之後，就得到了輸出的刺激強度。
![NeuralNet1](https://user-images.githubusercontent.com/62127656/115643086-b71f7200-a34e-11eb-8699-301484859b3d.jpg)
* 當然、神經網路並不是「神奇銀彈」，可以解決人工智慧上的所有問題，神經網路最強大的地方是容錯性很強，而且不需要像專家系統這樣撰寫一堆規則，但是有一得必有一失，神經網路自動學習完成之後，我們根本不知道該如何再去改進這個學習成果，因為那些權重對人類來說根本就沒有什麼直觀的意義，因此也就很難再去改進這個網路了。
  不過、程式能夠自我學習畢竟是一件很神奇的事情，光是這點就值得讓我們好好的去瞭解一下神經網路到底是怎麼運作的了！
## 從微分到梯度下降法
![maxresdefault](https://user-images.githubusercontent.com/62127656/115644370-0c5c8300-a351-11eb-9ba8-77987314af23.jpg)

### 單變數微分
#### 微分單變數
* [ diff.py ](https://gitlab.com/ccc109/ai/-/blob/master/07-neural/02-gradient/01-diff/diff.py)
```
PS C:\Users\rick2\ai\07-neural\02-gradient\01-diff> python diff.py
diff(f,2)= 4.000999999999699
```
* [e.py](https://github.com/cycyucheng1010/ai109b/blob/main/Homework/e.py)
```
n= 100.0 e(n)= 2.7048138294215285
n= 200.0 e(n)= 2.711517122929317 
n= 300.0 e(n)= 2.7137651579427837
n= 400.0 e(n)= 2.7148917443812293
n= 500.0 e(n)= 2.715568520651728 
n= 600.0 e(n)= 2.7160200488806514
n= 700.0 e(n)= 2.7163427377295566
n= 800.0 e(n)= 2.716584846682471
n= 900.0 e(n)= 2.716773208380411
n= 1000.0 e(n)= 2.7169239322355936
.
.
.
n= 9000.0 e(n)= 2.7181308281830128
n= 9100.0 e(n)= 2.718132487359168
n= 9200.0 e(n)= 2.718134110467929
n= 9300.0 e(n)= 2.718135698675662
n= 9400.0 e(n)= 2.718137253097062
n= 9500.0 e(n)= 2.7181387748001744
n= 9700.0 e(n)= 2.718141724076723
n= 9800.0 e(n)= 2.718143153583405
n= 9900.0 e(n)= 2.718144554210053
n= 10000.0 e(n)= 2.7181459268249255
```

### 梯度下降法
* 深度學習 (Deep Learning) 是人工智慧領域當紅的技術，說穿了其實就是原本的《神經網路》(Neural Network) ，不過由於加上了一些新的模型 (像是捲積神經網路 CNN, 循環神經網路 RNN 與生成對抗網路 GAN)，還有在神經網路的層數上加深很多，從以往的 3-4 層，提升到了十幾層，甚至上百層，於是我們給這些新一代的《神經網路》技術一個統稱，那就是《深度學習》。
  雖然《深度學習》的神經網路層數變多了，《網路模型》也多了一些，但是背後的學習算法和運作原理並沒有多大改變，仍然是以《梯度下降》(Gradient Descendent) 和《反傳遞算法》(Back Propagation) 為主。
* 梯度就是斜率最大的那個方向，所以梯度下降法，其實就是朝著斜率最大的方向走。
![Gradient](https://user-images.githubusercontent.com/62127656/115646825-70814600-a355-11eb-80a8-707ae152eaf2.jpg)
>往箭頭方向則梯度大，反之則小
---
* 讓我們先回頭看看梯度中的基本元素，也就是偏微分，其定義是：
```math
\frac{\partial }{\partial x_1} f(x) = \lim_{h \to 0} \frac{f(x_1, ..., x_i+h, ...., x_n)-f(x_1, ..., x_i, ...., x_n)}{h}
```

舉例而言，假如對 $`f(x,y) = x^2+y^2`$  這個函數而言，其對 x 的偏微分就是：

```math
\frac{\partial }{\partial x} f(x,y) = \lim_{h \to 0} \frac{f(x+h,y)-f(x,y)}{h}
```

而對 y 的偏微分就是：

```math
\frac{\partial }{\partial y} f(x,y) = \lim_{h \to 0} \frac{f(x,y+h)-f(x,y)}{h}
```
* [gd1.py](https://gitlab.com/ccc109/ai/-/blob/master/07-neural/02-gradient/03-gd/gd1.py)
```
import numpy as np
from numpy.linalg import norm 

# 函數 f 對變數 k 的偏微分: df / dk
def df(f, p, k, step=0.01):
    p1 = p.copy()
    p1[k] = p[k]+step
    return (f(p1) - f(p)) / step

# 函數 f 在點 p 上的梯度
def grad(f, p, step=0.01):
    gp = p.copy()
    for k in range(len(p)):
        gp[k] = df(f, p, k, step)
    return gp

# 使用梯度下降法尋找函數最低點
def gradientDescendent(f, p0, step=0.01):
    p = p0.copy()
    i = 0
    while (True):
        i += 1
        fp = f(p)
        gp = grad(f, p) # 計算梯度 gp
        glen = norm(gp) # norm = 梯度的長度 (步伐大小)
        print('{:d}:p={:s} f(p)={:.3f} gp={:s} glen={:.5f}'.format(i, str(p), fp, str(gp), glen))
        if glen < 0.00001:  # 如果步伐已經很小了，那麼就停止吧！
            break
        gstep = np.multiply(gp, -1*step) # gstep = 逆梯度方向的一小步
        p +=  gstep # 向 gstep 方向走一小步
    return p # 傳回最低點！
```
* [ gdGate.py](https://gitlab.com/ccc109/ai/-/blob/master/07-neural/02-gradient/03-gd/gdGate.py)
```
import numpy as np
import math
import gd3 as gd

def sig(t):
    return 1.0/(1.0+math.exp(-t))

o = [0,0,0,1] # and gate outputs
# o = [0,1,1,1] # or gate outputs
# o = [0,1,1,0] # xor gate outputs
def loss(p, dump=False):
    [w1,w2,b] = p
    o0 = sig(w1*0+w2*0+b)
    o1 = sig(w1*0+w2*1+b)
    o2 = sig(w1*1+w2*0+b)
    o3 = sig(w1*1+w2*1+b)
    delta = np.array([o0-o[0], o1-o[1], o2-o[2], o3-o[3]])
    if dump:
        print('o0={:.3f} o1={:.3f} o2={:.3f} o3={:.3f}'.format(o0,o1,o2,o3))
    return np.linalg.norm(delta, 2)

p = [0.0, 0.0, 0.0] # [w1,w2,b] 
plearn = gd.gradientDescendent(loss, p, max_loops=3000)
loss(plearn, True)

```
* Sigmoid函數
![1200px-Logistic-curve svg](https://user-images.githubusercontent.com/62127656/115653626-133fc180-a362-11eb-9ce7-1888258f0f5f.png)
