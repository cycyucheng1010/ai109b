# 20210225第一週
## 評分方式
* 上課發問
* 筆記
* 專案
## 人工智慧課程地圖

主題                         | 內容
----------------------------|---------------------------------------------------------
[簡介](./01-introduction/)   | 簡介 / 歷史 / 現況
[優化算法](./02-optimize/)    | 爬山演算法 / 遺傳演算法 / 限制最佳化
[圖形搜尋](./03-search/)      | 圖形表示 / 解空間 / DFS / BFS / IDS / A*
[邏輯推論](./04-logic/)       | 布林邏輯 / 一階邏輯 / 自動推論 / 專家系統
[科學計算](./05-math/)       | 機率 / 統計 / 向量 / 矩陣 / 代數 / 幾何 / 微積分
[機器學習](./06-learn/)      | 馬可夫鏈 / HMM / EM / KNN / KMean
[神經網路](./07-neural/)      | 梯度下降法 / 反傳遞算法 / 感知器 / MLP
[深度學習](./08-deep/)        | 張量 / PyTorch / CNN / RNN / GAN / 強化學習
[影像處理](./09-image/)       | 影像辨識 / CNN / YoLo / 風格轉換 / 深度偽裝
[語言處理](./10-lang/)        | 查表法 / 遞迴下降 / 字串比對 / RNN / 詞向量 / seq2seq / BERT
[電腦下棋](./11-chess/)       | 對局搜尋 / 策略梯度下降 / 蒙地卡羅 / AlphaGo
[自動控制](./12-control/)     | 回饋系統 / 模糊控制 / 強化學習 / 自動駕駛 / 機器人

## 課程須知
* 使用語言及函式庫:python、pytorch、numpy等.....
## 傳統作法
* 搜尋+優化
## 機器學習
* 統計
* 神經網路 ------> 深度學習
## 應用
* 影像 -------> 特徵、CNN(YOLO)
* 語言 -------> 規則、統計、RNN
## 優化
* 數學 ------> 科學計算
1. 機率統計
2. 微積分
3. 代數(線性代數、泛函)
5. 幾何(微分幾何)
## 爬山演算法
* 爬山演算法 (Hill Climbing) 是一種最簡單的優化算法，該方法就像模擬人類爬山時的行為而設計的，因此稱為爬山演算法。
* 一直往低的地方走，一直走到最低點，然後你會看到左右兩邊都沒辦法更低了，於是就停止尋找，傳回該最低點作為答案。
* 如果我們想要找的是最高點，而不是最低點，那整個行為就會像爬山一樣，只是最後爬到山頂就會停了。<br>
co
```
# 簡易爬山演算法 -- 針對單變數函數
def hillClimbing(f, x, dx=0.01):
    while (True):
        print('x={0:.3f} f(x)={1:.3f}'.format(x, f(x)))
        if f(x+dx)>f(x): # 如果右邊的高度 f(x+dx) > 目前高度 f(x) ，那麼就往右走
            x = x + dx
        elif f(x-dx)>f(x): # 如果左邊的高度 f(x-dx) > 目前高度 f(x) ，那麼就往左走
            x = x - dx
        else: # 如果兩邊都沒有比現在的 f(x) 高，那麼這裡就是區域最高點，直接中斷傳回
            break
    return x

# 高度函數
def f(x):
    return -1*(x*x-2*x+1)
    # return -1*(x*x+3*x+5)
    # return -1*abs(x*x-4)

hillClimbing(f, 0) # 以 x=0 為起點，開始呼叫爬山演算法
```
> 利用高度找出Ans

code1 Result
```
PS C:\Users\rick2\ai\02-optimize\01-hillclimbing\02-var1> python hillClimbing1.py
x=0.000 f(x)=-1.000
x=0.010 f(x)=-0.980
x=0.020 f(x)=-0.960
x=0.030 f(x)=-0.941
x=0.040 f(x)=-0.922
x=0.050 f(x)=-0.902
.
.
.
x=0.950 f(x)=-0.002
x=0.960 f(x)=-0.002
x=0.970 f(x)=-0.001
x=0.980 f(x)=-0.000
x=0.990 f(x)=-0.000
x=1.000 f(x)=-0.000
```
code2
```
import random

def hillClimbing(f, x, y, h=0.01):
    while (True):
        fxy = f(x, y)
        print('x={0:.3f} y={1:.3f} f(x,y)={2:.3f}'.format(x, y, fxy))
        if f(x+h, y) >= fxy:
            x = x + h
        elif f(x-h, y) >= fxy:
            x = x - h
        elif f(x, y+h) >= fxy:
            y = y + h
        elif f(x, y-h) >= fxy:
            y = y - h
        else:
            break
    return (x,y,fxy)

def f(x, y):
    return -1 * ( x*x - 2*x + y*y + 2*y - 8 )

hillClimbing(f, 0, 0)
```
code2result
```
PS C:\Users\rick2\ai\02-optimize\01-hillclimbing\03-var2> python hillClimbing2.py
x=0.000 y=0.000 f(x,y)=8.000
x=0.010 y=0.000 f(x,y)=8.020
x=0.020 y=0.000 f(x,y)=8.040
x=0.030 y=0.000 f(x,y)=8.059
x=0.040 y=0.000 f(x,y)=8.078
x=0.050 y=0.000 f(x,y)=8.098
.
.
.
x=1.000 y=-0.950 f(x,y)=9.998
x=1.000 y=-0.960 f(x,y)=9.998
x=1.000 y=-0.970 f(x,y)=9.999
x=1.000 y=-0.980 f(x,y)=10.000
x=1.000 y=-0.990 f(x,y)=10.000
x=1.000 y=-1.000 f(x,y)=10.000
```
